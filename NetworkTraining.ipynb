{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8b77197",
   "metadata": {},
   "source": [
    "<img src = \"../PES.jpeg\" alt=\"PES\" class=\"bg-primary\" height= \"75px\" width=\"50px\" align = \"left\">\n",
    "\n",
    "<img src = \"../CIE.jpeg\" alt=\"CIE\" class=\"bg-primary\"  height= \"100px\" width=\"200px\" align = \"right\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067aed0",
   "metadata": {},
   "source": [
    "&copy; Center For Innovation and Entrepreneurship, PES University\n",
    "\n",
    "Author: Dheemanth R Joshi\n",
    "\n",
    "Research Assistant, CIE PES University\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb2bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for arrays\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "\"\"\"for plotting images\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\"\"\"Parent Library of the PyTorch Framework\"\"\"\n",
    "\n",
    "import torch \n",
    "\n",
    "\"\"\"Neural Network library of the pytorch module\"\"\"\n",
    "\n",
    "import torch.nn as nn \n",
    "\n",
    "\"\"\"Torch Dataset Libraries\"\"\"\n",
    "\n",
    "import torchvision \n",
    "\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3514a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains Network on GPU if Available\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a25334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Network and training parameters for this dataset\"\"\"\n",
    "\n",
    "input_size= 784\n",
    "\n",
    "batch_size= 100 \n",
    "\n",
    "num_classes= 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b74c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import the MNIST Dataset and setup the dataloader\"\"\"\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('./data', train = True, \n",
    "                                           transform= transforms.ToTensor(), download = True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST('./data', train = False, \n",
    "                                           transform= transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size=batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83891db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdbd8076d60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"View the test dataset\"\"\"\n",
    "\n",
    "image_index = 4\n",
    "\n",
    "plt.imshow(test_dataset[image_index][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951a1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save test image for inference\"\"\"\n",
    "\n",
    "test_image = test_dataset[image_index][0][0].numpy()\n",
    "\n",
    "np.savetxt(\"Desktop/TrainedNetwork/testimage.txt\", test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0fefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the Neural Network Architecture\"\"\"\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.InputLayer = nn.Linear(input_size, 100)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.hl = nn.Linear(100, 100)\n",
    "        \n",
    "        self.OutputLayer = nn.Linear(100, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        response = self.InputLayer(x)\n",
    "        \n",
    "        response = self.relu(response)\n",
    "        \n",
    "        response = self.hl(response)\n",
    "        \n",
    "        response = self.relu(response)\n",
    "        \n",
    "        response = self.OutputLayer(response)\n",
    "        \n",
    "        return response \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47aca85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training: Define Loss and Optimizer\"\"\"\n",
    "\n",
    "model = NeuralNet(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0529954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training Cond...\"\"\"\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outs = model(images)\n",
    "        \n",
    "        loss = criterion(outs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \"\"\"Remove comments to view the epoch v/s loss\"\"\"\n",
    "        \n",
    "        \"\"\"if((i+1)%100 == 0):\n",
    "            \n",
    "            print(f\"epoch{epoch+1}, step{i+1}, loss= {loss.item()}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c676aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 97.46000000000001 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Model Accuracy Testing\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    corrects = 0\n",
    "    \n",
    "    n_samples = 0 \n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        \n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outs, 1)\n",
    "        \n",
    "        n_samples += labels.size(0)\n",
    "        \n",
    "        corrects += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100.0 * (corrects/n_samples)\n",
    "    \n",
    "    print(f\"Accuracy = {accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac34b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    input_layer_weights = model.InputLayer.weight.numpy()\n",
    "    \n",
    "    input_layer_biases = model.InputLayer.bias.numpy()\n",
    "    \n",
    "    hl_layer_weights = model.hl.weight.numpy()\n",
    "    \n",
    "    hl_layer_biases = model.hl.bias.numpy()\n",
    "    \n",
    "    output_layer_weights = model.OutputLayer.weight.numpy()\n",
    "    \n",
    "    output_layer_biases = model.OutputLayer.bias.numpy()\n",
    "    \n",
    "    np.savetxt(\"Desktop/TrainedNetwork/in_layer_weights.txt\", input_layer_weights)\n",
    "    \n",
    "    np.savetxt(\"Desktop/TrainedNetwork/in_layer_biases.txt\", input_layer_biases)\n",
    "    \n",
    "    np.savetxt(\"Desktop/TrainedNetwork/hl_layer_weights.txt\", hl_layer_weights)\n",
    "    \n",
    "    np.savetxt(\"Desktop/TrainedNetwork/hl_layer_biases.txt\", hl_layer_biases)\n",
    "    \n",
    "    np.savetxt(\"Desktop/TrainedNetwork/out_layer_weights.txt\", output_layer_weights)\n",
    "    \n",
    "    np.savetxt(\"Desktop/TrainedNetwork/out_layer_biases.txt\", output_layer_biases)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
